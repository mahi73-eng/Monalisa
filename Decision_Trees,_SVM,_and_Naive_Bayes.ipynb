{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 :  What is Information Gain, and how is it used in Decision Trees?"
      ],
      "metadata": {
        "id": "aP3goejNgLCj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a841972"
      },
      "source": [
        "Information Gain is a measure used in the construction of Decision Trees. It quantifies the reduction in entropy (or impurity) of a dataset after a split based on an attribute.\n",
        "\n",
        "**How it's used in Decision Trees:**\n",
        "\n",
        "Decision Trees are built by recursively splitting the dataset based on the attribute that provides the highest Information Gain. At each node of the tree, the algorithm considers all available attributes and calculates the Information Gain for splitting the data based on each attribute's values. The attribute that results in the largest Information Gain is chosen as the splitting criterion for that node. This process is repeated until the tree is fully grown or a stopping criterion is met.\n",
        "\n",
        "In essence, Information Gain helps the Decision Tree algorithm select the most informative features to split the data, leading to a tree that can effectively classify or predict the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases."
      ],
      "metadata": {
        "id": "hBdamKKqgXKj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa55dd4"
      },
      "source": [
        "Both Gini Impurity and Entropy are metrics used to measure the impurity or randomness of a dataset in the context of Decision Trees. They help determine the effectiveness of splitting a node based on a particular attribute.\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "*   Measures the probability of misclassifying a randomly chosen element in the dataset if it were labeled randomly according to the distribution of labels in the subset.\n",
        "*   A value of 0 indicates perfect purity (all elements belong to the same class).\n",
        "*   Higher values indicate greater impurity.\n",
        "*   Computationally less expensive than Entropy as it doesn't involve logarithms.\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "*   Measures the average amount of information or uncertainty in the dataset.\n",
        "*   A value of 0 indicates perfect purity.\n",
        "*   Higher values indicate greater impurity.\n",
        "*   Based on information theory and uses logarithms in its calculation.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "*   **Calculation:** Gini Impurity uses squared probabilities, while Entropy uses logarithms.\n",
        "*   **Sensitivity:** Entropy tends to be slightly more sensitive to changes in the class distribution than Gini Impurity.\n",
        "*   **Computational Cost:** Gini Impurity is generally faster to compute due to the absence of logarithms.\n",
        "\n",
        "**Use Cases:**\n",
        "\n",
        "*   Both are commonly used in Decision Tree algorithms like CART (Classification and Regression Trees).\n",
        "*   In practice, the choice between Gini Impurity and Entropy often has little impact on the final tree structure and performance.\n",
        "*   Gini Impurity is the default impurity measure in many implementations (e.g., scikit-learn's `DecisionTreeClassifier`).\n",
        "\n",
        "In summary, both Gini Impurity and Entropy serve the same purpose of quantifying impurity in Decision Trees. The choice between them is often a matter of computational efficiency or slight theoretical preferences, with Gini Impurity being slightly faster to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3:What is Pre-Pruning in Decision Trees?"
      ],
      "metadata": {
        "id": "aomLWrn-hAOr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "294b51b0"
      },
      "source": [
        "Pre-pruning is a technique used in the construction of Decision Trees to stop the tree building process early, before it has perfectly classified the training data. The goal is to prevent overfitting by limiting the complexity of the tree.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "During the tree building process, at each node, the algorithm checks if splitting the node would improve the model's performance on a validation set or if it meets a certain predefined criterion. If splitting does not lead to significant improvement or violates the criterion, the node is not split further and becomes a leaf node.\n",
        "\n",
        "**Common Pre-pruning Criteria:**\n",
        "\n",
        "*   **Maximum Depth:** Limiting the maximum depth of the tree.\n",
        "*   **Minimum Samples Split:** Requiring a minimum number of samples in a node before it can be split.\n",
        "*   **Minimum Samples Leaf:** Requiring a minimum number of samples in a leaf node.\n",
        "*   **Maximum Leaf Nodes:** Limiting the total number of leaf nodes in the tree.\n",
        "*   **Information Gain Threshold:** Requiring a minimum information gain to justify a split.\n",
        "\n",
        "**Advantages of Pre-pruning:**\n",
        "\n",
        "*   **Faster Training:** Stops the tree building process earlier, leading to faster training times.\n",
        "*   **Reduced Overfitting:** Prevents the tree from becoming too complex and fitting the training data too closely, which can improve generalization to unseen data.\n",
        "*   **Simpler Trees:** Results in smaller and more interpretable trees.\n",
        "\n",
        "**Disadvantages of Pre-pruning:**\n",
        "\n",
        "*   **Greedy Approach:** Once a node is not split, that decision is final, even if a better split might have been possible at a later stage. This can lead to the \"horizon effect,\" where the algorithm stops too early.\n",
        "*   **Difficulty in Choosing Criteria:** Selecting the appropriate pre-pruning criteria and their values can be challenging and often requires experimentation.\n",
        "\n",
        "In summary, pre-pruning is a regularization technique that aims to simplify Decision Trees and prevent overfitting by stopping the tree growth early based on predefined criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "-MwQGG-yhFLk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fbb87f5",
        "outputId": "7e2a5ed5-8f9a-4430-d349-fb91f2b0cd7e"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with Gini impurity\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization of feature importances\n",
        "feature_importances_series = pd.Series(feature_importances, index=X.columns)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importances_series.sort_values(ascending=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "petal length (cm)    0.906143\n",
            "petal width (cm)     0.077186\n",
            "sepal width (cm)     0.016670\n",
            "sepal length (cm)    0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "g2-QfxweieUs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7824f2cc"
      },
      "source": [
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for both classification and regression tasks. However, it is primarily used for classification.\n",
        "\n",
        "**Key Concepts of SVM:**\n",
        "\n",
        "*   **Hyperplane:** In a classification problem, an SVM aims to find the optimal hyperplane that separates the data points of different classes in a high-dimensional space. For a 2D dataset with two classes, this hyperplane is a line. For a 3D dataset, it's a plane, and for higher dimensions, it's called a hyperplane.\n",
        "*   **Support Vectors:** These are the data points from the training set that are closest to the hyperplane. They play a crucial role in defining the hyperplane and the margin. Only these points are relevant for the construction of the classifier.\n",
        "*   **Margin:** The margin is the distance between the hyperplane and the nearest data points (support vectors) from each class. SVMs aim to maximize this margin. A larger margin is generally associated with better generalization performance.\n",
        "\n",
        "**How SVM Works:**\n",
        "\n",
        "The goal of an SVM is to find the hyperplane that maximizes the margin between the two classes. This is achieved by solving an optimization problem. For linearly separable data, the SVM finds a linear hyperplane.\n",
        "\n",
        "**Handling Non-linearly Separable Data:**\n",
        "\n",
        "In many real-world scenarios, data is not linearly separable. SVMs handle this by using the **kernel trick**. The kernel trick allows SVMs to implicitly map the data into a higher-dimensional space where it might be linearly separable, without actually computing the coordinates in that higher dimension. Common kernel functions include:\n",
        "\n",
        "*   **Linear Kernel:** Suitable for linearly separable data.\n",
        "*   **Polynomial Kernel:** Maps data into a higher-dimensional space using polynomial combinations of the original features.\n",
        "*   **Radial Basis Function (RBF) Kernel:** A popular choice that can map data into an infinite-dimensional space.\n",
        "\n",
        "**Advantages of SVM:**\n",
        "\n",
        "*   **Effective in high-dimensional spaces:** SVMs work well in datasets with many features.\n",
        "*   **Memory efficient:** Because they only use a subset of training data (support vectors) in the decision function.\n",
        "*   **Versatile:** Different kernel functions can be used for various datasets.\n",
        "\n",
        "**Disadvantages of SVM:**\n",
        "\n",
        "*   **Computationally expensive:** Can be slow on large datasets.\n",
        "*   **Sensitive to noisy data and outliers:** Outliers can significantly impact the hyperplane and margin.\n",
        "*   **Difficulty in interpreting the model:** Unlike decision trees, understanding the influence of individual features can be challenging.\n",
        "\n",
        "In summary, SVMs are powerful algorithms that find an optimal hyperplane to separate data classes, using the concept of maximizing the margin and employing the kernel trick to handle non-linearly separable data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6:  What is the Kernel Trick in SVM?"
      ],
      "metadata": {
        "id": "8fQusooZj9OQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f051a8"
      },
      "source": [
        "The **Kernel Trick** is a powerful technique used in Support Vector Machines (SVMs) to handle non-linearly separable data. It allows SVMs to find a linear separating hyperplane in a higher-dimensional feature space without explicitly transforming the data into that space.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "* **The Problem:** In many real-world datasets, the data points belonging to different classes cannot be separated by a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions). If the data is not linearly separable, a linear SVM cannot find a suitable decision boundary.\n",
        "\n",
        "* **The Idea:** The core idea behind the kernel trick is to map the original data into a higher-dimensional space where it becomes linearly separable. In this higher-dimensional space, it might be possible to find a hyperplane that effectively separates the classes.\n",
        "\n",
        "* **The \"Trick\":** Instead of explicitly performing this mapping (which can be computationally expensive, especially for very high or infinite dimensions), the kernel trick uses a **kernel function**. A kernel function is a function that calculates the dot product of the data points in the higher-dimensional space directly from their original coordinates.\n",
        "\n",
        "Mathematically, if $\\phi(x)$ is the mapping function that transforms data from the original space to the higher-dimensional space, the kernel function $K(x_i, x_j)$ is defined as:\n",
        "\n",
        "$K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$\n",
        "\n",
        "where $x_i$ and $x_j$ are data points in the original space.\n",
        "\n",
        "The SVM algorithm, which relies on dot products between data points, can then use this kernel function to compute the dot products in the higher-dimensional space without ever explicitly computing $\\phi(x)$. This significantly reduces the computational cost.\n",
        "\n",
        "**Common Kernel Functions:**\n",
        "\n",
        "* **Linear Kernel:** $K(x_i, x_j) = x_i \\cdot x_j$ (equivalent to no mapping, used for linearly separable data)\n",
        "* **Polynomial Kernel:** $K(x_i, x_j) = (x_i \\cdot x_j + c)^d$ (maps data using polynomial combinations)\n",
        "* **Radial Basis Function (RBF) Kernel:** $K(x_i, x_j) = exp(-\\gamma \\|x_i - x_j\\|^2)$ (a popular choice that can map data to an infinite-dimensional space)\n",
        "\n",
        "**Benefits of the Kernel Trick:**\n",
        "\n",
        "* **Handles Non-linear Data:** Allows SVMs to classify data that is not linearly separable in the original space.\n",
        "* **Computational Efficiency:** Avoids explicit computation in high-dimensional spaces, making it feasible to work with complex data.\n",
        "* **Flexibility:** Different kernel functions can be chosen based on the characteristics of the data.\n",
        "\n",
        "In essence, the kernel trick provides a computationally efficient way for SVMs to operate in high-dimensional feature spaces, enabling them to solve non-linear classification problems effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset."
      ],
      "metadata": {
        "id": "SZo2ZCwAmhAK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ace537f",
        "outputId": "5eaab8c0-43bf-44bd-eb77-efc8c0fea8fb"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the SVM classifier with Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for Linear kernel\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Initialize and train the SVM classifier with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set and calculate accuracy for RBF kernel\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 1.0000\n",
            "Accuracy of SVM with RBF Kernel: 0.8056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ],
      "metadata": {
        "id": "9hH2y7bVmvNS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaceff6d"
      },
      "source": [
        "The **Naïve Bayes classifier** is a supervised machine learning algorithm based on **Bayes' Theorem**. It is primarily used for classification tasks.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "The core idea behind Naïve Bayes is to calculate the probability of a given data point belonging to a particular class based on the probabilities of its features. It uses Bayes' Theorem, which is expressed as:\n",
        "\n",
        "$P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}$\n",
        "\n",
        "Where:\n",
        "* $P(A|B)$: The probability of event A happening given that event B has happened (this is the posterior probability we want to calculate).\n",
        "* $P(B|A)$: The probability of event B happening given that event A has happened (likelihood).\n",
        "* $P(A)$: The prior probability of event A.\n",
        "* $P(B)$: The prior probability of event B.\n",
        "\n",
        "In the context of classification, we want to find the probability of a data point belonging to a class $C$ given its features $F_1, F_2, ..., F_n$. Using Bayes' Theorem, this can be written as:\n",
        "\n",
        "$P(C | F_1, F_2, ..., F_n) = \\frac{P(F_1, F_2, ..., F_n | C) * P(C)}{P(F_1, F_2, ..., F_n)}$\n",
        "\n",
        "The classifier then predicts the class that has the highest posterior probability.\n",
        "\n",
        "**Why is it called \"Naïve\"?**\n",
        "\n",
        "The \"Naïve\" part of the name comes from the **simplifying assumption** that the features are **independent** of each other, given the class. This means the algorithm assumes that the presence or absence of a particular feature does not affect the presence or absence of any other feature, given the class.\n",
        "\n",
        "Mathematically, this assumption allows us to simplify the likelihood term:\n",
        "\n",
        "$P(F_1, F_2, ..., F_n | C) = P(F_1 | C) * P(F_2 | C) * ... * P(F_n | C)$\n",
        "\n",
        "This independence assumption is often not true in real-world datasets, where features can be correlated. However, despite this simplification, Naïve Bayes classifiers often perform surprisingly well in practice, especially with large datasets.\n",
        "\n",
        "**Advantages of Naïve Bayes:**\n",
        "\n",
        "* **Simple and easy to implement:** The underlying concepts are straightforward.\n",
        "* **Fast training and prediction:** Due to the independence assumption, calculations are efficient.\n",
        "* **Works well with high-dimensional data:** Effective for text classification with many features (words).\n",
        "* **Requires less training data:** Can perform reasonably well even with a limited amount of data compared to some other algorithms.\n",
        "\n",
        "**Disadvantages of Naïve Bayes:**\n",
        "\n",
        "* **Naïve independence assumption:** The assumption of independent features is often violated in reality, which can affect performance.\n",
        "* **Zero probability problem:** If a feature value does not appear in the training data for a particular class, the probability for that feature given the class will be zero, which can make the entire posterior probability zero. This is often addressed using smoothing techniques like Laplace smoothing.\n",
        "\n",
        "In summary, the Naïve Bayes classifier is a simple yet effective algorithm based on Bayes' Theorem. Its \"Naïve\" nature stems from the assumption of feature independence, which simplifies the calculations but may not always hold true in real-world data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes"
      ],
      "metadata": {
        "id": "amFtAGaXnZj-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a38fbfac"
      },
      "source": [
        "The Naïve Bayes algorithm has several variations, primarily differing in the assumption they make about the distribution of the features. The three most common types are Gaussian, Multinomial, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Here are the key differences:\n",
        "\n",
        "**1. Gaussian Naïve Bayes:**\n",
        "\n",
        "* **Assumption:** Assumes that the continuous features associated with each class are distributed according to a **Gaussian (Normal) distribution**.\n",
        "* **Use Case:** Suitable for datasets where features are continuous and can be reasonably assumed to follow a normal distribution (e.g., measurements, sensor readings).\n",
        "* **How it works:** It calculates the mean and standard deviation of each feature for each class during training. During prediction, it uses these statistics to calculate the probability of a given feature value occurring for each class, assuming a Gaussian distribution.\n",
        "\n",
        "**2. Multinomial Naïve Bayes:**\n",
        "\n",
        "* **Assumption:** Assumes that the features represent the **counts** or **frequencies** of events. It is typically used for discrete data, such as word counts in text classification.\n",
        "* **Use Case:** Widely used in Natural Language Processing (NLP) for tasks like document classification, spam filtering, and sentiment analysis, where features are word counts or term frequencies.\n",
        "* **How it works:** It calculates the probability of observing a particular count for a feature given a class, based on the frequency of that feature in the training data for that class.\n",
        "\n",
        "**3. Bernoulli Naïve Bayes:**\n",
        "\n",
        "* **Assumption:** Assumes that the features are **binary** (Boolean) variables, meaning they represent the presence or absence of a particular event or feature.\n",
        "* **Use Case:** Also used in text classification, but specifically for datasets where the features are binary indicators of whether a word is present or absent in a document, rather than its count. It can also be used for other binary feature datasets.\n",
        "* **How it works:** It calculates the probability of a binary feature being present or absent given a class, based on the proportion of documents in the training data for that class that contain (or do not contain) that feature.\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Feature         | Gaussian Naïve Bayes                 | Multinomial Naïve Bayes                | Bernoulli Naïve Bayes                   |\n",
        "|-----------------|--------------------------------------|----------------------------------------|-----------------------------------------|\n",
        "| **Feature Type** | Continuous                           | Discrete (counts, frequencies)         | Binary (presence/absence)               |\n",
        "| **Distribution**| Gaussian (Normal)                    | Multinomial                            | Bernoulli                               |\n",
        "| **Use Cases**   | Continuous data, measurements         | Text classification (word counts)      | Text classification (binary word feats), binary data |\n",
        "\n",
        "Choosing the appropriate type of Naïve Bayes depends on the nature of your data and the type of features you have."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10:  Breast Cancer Dataset\n",
        "# Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets."
      ],
      "metadata": {
        "id": "8NU1wTujoTtr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03b4c9a1",
        "outputId": "741a301a-4fa4-40d7-bda3-81f4f84ee6c7"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes Classifier on Breast Cancer dataset: {accuracy:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes Classifier on Breast Cancer dataset: 0.9737\n"
          ]
        }
      ]
    }
  ]
}